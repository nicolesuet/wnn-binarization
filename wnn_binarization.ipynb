{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP9Ds582fjeEUVvuaujq+cc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolesuet/wnn-binarization/blob/main/wnn_binarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.functional import cross_entropy\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_dwn as dwn\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchhd import embeddings"
      ],
      "metadata": {
        "id": "81Uqzt0m4eyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "hRCWZrPF8DwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = fetch_ucirepo(id=53)\n",
        "X = iris.data.features\n",
        "y = iris.data.targets"
      ],
      "metadata": {
        "id": "42wcdzQg7g1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_global = np.array(X.values).flatten().min()\n",
        "max_global = np.array(X.values).flatten().max()\n",
        "\n",
        "print(f\"Min global: {min_global}\")\n",
        "print(f\"Max global: {max_global}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9ELiyl47mcx",
        "outputId": "61d9c29b-c35b-4d70-8c49-3f3d54b83279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min global: 0.1\n",
            "Max global: 7.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "y_train = encoder.fit_transform(y_train.values.ravel())\n",
        "y_test = encoder.fit_transform(y_test.values.ravel())\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)"
      ],
      "metadata": {
        "id": "7ZhkX6gZ7nVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_tensor = torch.tensor(X.values)\n",
        "print(torch_tensor[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UikCNRv78I_P",
        "outputId": "c832734b-4ded-4e65-8326-420c8f611370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5.1000, 3.5000, 1.4000, 0.2000],\n",
            "        [4.9000, 3.0000, 1.4000, 0.2000],\n",
            "        [4.7000, 3.2000, 1.3000, 0.2000],\n",
            "        [4.6000, 3.1000, 1.5000, 0.2000],\n",
            "        [5.0000, 3.6000, 1.4000, 0.2000]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoders = [\n",
        "    {\"encoding\": \"DISTRIBUTIVE\", \"encoder\": dwn.DistributiveThermometer(10).fit(torch_tensor)},\n",
        "    {\"encoding\": \"GAUSSIAN\", \"encoder\": dwn.GaussianThermometer(10).fit(torch_tensor)},\n",
        "]\n",
        "\n",
        "ADDRESS_SIZE = 10\n",
        "IGNORE_ZERO = False\n",
        "VERBOSE = False"
      ],
      "metadata": {
        "id": "2DywxztC8QH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, x_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = (model(x_test.cuda(device)).cpu()).argmax(dim=1).numpy()\n",
        "        acc = (pred == y_test.numpy()).sum() / y_test.shape[0]\n",
        "    return acc"
      ],
      "metadata": {
        "id": "AgRVe0vHB1N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for elem in encoders:\n",
        "    print(f\"\\nEncoding: {elem['encoding']}\\n\")\n",
        "\n",
        "    encoder = elem[\"encoder\"]\n",
        "\n",
        "    x_train = encoder.binarize(torch.tensor(X_train.values)).flatten(start_dim=1)\n",
        "    x_test = encoder.binarize(torch.tensor(X_test.values)).flatten(start_dim=1)\n",
        "    X_bin = encoder.binarize(torch.tensor(X.values)).flatten(start_dim=1)\n",
        "\n",
        "\n",
        "    model = nn.Sequential(\n",
        "      dwn.LUTLayer(x_train.size(1), 2000, n=6, mapping='learnable'),\n",
        "      dwn.LUTLayer(2000, 1000, n=6),\n",
        "      dwn.GroupSum(k=10, tau=1/0.3)\n",
        "    )\n",
        "\n",
        "    model = model.cuda()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=14)\n",
        "\n",
        "    epochs = 1\n",
        "    n_samples = x_train.shape[0]\n",
        "    batch_size = 32\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      print(f\"Epoch: {epoch}\")\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      permutation = torch.randperm(n_samples)\n",
        "      correct_train = 0\n",
        "      total_train = 0\n",
        "\n",
        "      for i in range(0, n_samples, batch_size):\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          indices = permutation[i:i+batch_size]\n",
        "          batch_x, batch_y = x_train[indices].cuda(device), y_train_tensor[indices].cuda(device)\n",
        "          outputs = model(batch_x)\n",
        "          loss = cross_entropy(outputs, batch_y)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          pred_train = outputs.argmax(dim=1)\n",
        "          correct_train += (pred_train == batch_y).sum().item()\n",
        "          total_train += batch_y.size(0)\n",
        "\n",
        "      train_acc = correct_train / total_train\n",
        "      scheduler.step()\n",
        "      test_acc = evaluate(model, x_test, y_test_tensor)\n",
        "      print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j88s9cuo8jtx",
        "outputId": "ee949fb4-1fe0-4aaf-92bd-faf7f5001191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoding: DISTRIBUTIVE\n",
            "\n",
            "Epoch: 0\n",
            "Epoch 1/1, Train Loss: 0.1461, Train Accuracy: 0.5900, Test Accuracy: 0.9000\n",
            "\n",
            "Encoding: GAUSSIAN\n",
            "\n",
            "Epoch: 0\n",
            "Epoch 1/1, Train Loss: 0.3758, Train Accuracy: 0.5200, Test Accuracy: 0.9400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    print(f\"\\nEncoding: SCATTER\\n\")\n",
        "\n",
        "    emb = embeddings.Level(10, 20, \"BSC\", low=min_global, high=max_global, dtype=torch.uint8)\n",
        "\n",
        "    x_train = emb(torch.tensor(X_train.values)).flatten(start_dim=1).float()\n",
        "    x_test = emb(torch.tensor(X_test.values)).flatten(start_dim=1).float()\n",
        "    X_bin = emb(torch.tensor(X.values)).flatten(start_dim=1).float()\n",
        "\n",
        "    model = nn.Sequential(\n",
        "      dwn.LUTLayer(x_train.size(1), 2000, n=6, mapping='learnable'),\n",
        "      dwn.LUTLayer(2000, 1000, n=6),\n",
        "      dwn.GroupSum(k=10, tau=1/0.3)\n",
        "    )\n",
        "\n",
        "    model = model.cuda()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=14)\n",
        "\n",
        "    epochs = 1\n",
        "    n_samples = x_train.shape[0]\n",
        "    batch_size = 32\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      print(f\"Epoch: {epoch}\")\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      permutation = torch.randperm(n_samples)\n",
        "      correct_train = 0\n",
        "      total_train = 0\n",
        "\n",
        "      for i in range(0, n_samples, batch_size):\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          indices = permutation[i:i+batch_size]\n",
        "          batch_x, batch_y = x_train[indices].cuda(device), y_train_tensor[indices].cuda(device)\n",
        "          outputs = model(batch_x)\n",
        "          loss = cross_entropy(outputs, batch_y)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          pred_train = outputs.argmax(dim=1)\n",
        "          correct_train += (pred_train == batch_y).sum().item()\n",
        "          total_train += batch_y.size(0)\n",
        "\n",
        "      train_acc = correct_train / total_train\n",
        "      scheduler.step()\n",
        "      test_acc = evaluate(model, x_test, y_test_tensor)\n",
        "      print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRDpyUcMDXw_",
        "outputId": "71678c67-2010-400b-cda4-cd9a5b8855ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoding: SCATTER\n",
            "\n",
            "Epoch: 0\n",
            "Epoch 1/1, Train Loss: 0.5944, Train Accuracy: 0.5000, Test Accuracy: 0.9000\n"
          ]
        }
      ]
    }
  ]
}